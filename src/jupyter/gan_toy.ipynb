{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_utils as tf_utils\n",
    "import utils as utils\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 256\n",
      "\tCRITIC_ITERS: 1\n",
      "\tDIM: 512\n",
      "\tDIS_DIM: 1\n",
      "\tFIXED_GENERATOR: False\n",
      "\tFREQ: 1000\n",
      "\tGEN_DIM: 2\n",
      "\tITERS: 100000\n",
      "\tLAMBDA: 0.1\n"
     ]
    }
   ],
   "source": [
    "DIM = 512  # model dimensionality\n",
    "GEN_DIM = 2  # output dimension of the generator\n",
    "DIS_DIM = 1  # outptu dimension fo the discriminator\n",
    "FIXED_GENERATOR = False  # wheter to hold the generator fixed at ral data plus Gaussian noise, as in the plots in the paper\n",
    "LAMBDA = .1  # smaller lambda makes things faster for toy tasks, but isn't necessary if you increase CRITIC_ITERS enough\n",
    "BATCH_SIZE = 256  # batch size\n",
    "ITERS = 100000  # how many generator iterations to train for\n",
    "FREQ = 1000  # sample frequency\n",
    "\n",
    "mode = 'wgan-g'  # [gan, wgan, wgan-gp]\n",
    "dataset = 'swissroll'  # [8gaussians, 25gaussians, swissroll]\n",
    "img_folder = os.path.join('img', mode + '_' + dataset + '_' + str(FIXED_GENERATOR))\n",
    "\n",
    "if mode == 'gan':\n",
    "    CRITIC_ITERS = 1  # homw many critic iteractions per generator iteration\n",
    "else:\n",
    "    CRITIC_ITERS = 5  # homw many critic iteractions per generator iteration\n",
    "\n",
    "if not os.path.isdir(img_folder):\n",
    "    os.makedirs(img_folder)\n",
    "\n",
    "utils.print_model_setting(locals().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen/fc-1/add   [256, 512]\n",
      "gen/fc-2/add   [256, 512]\n",
      "gen/fc-3/add   [256, 512]\n",
      "gen/fc-4/add   [256, 2]\n",
      "is_reuse: False\n",
      "disc/fc-1/add   [None, 512]\n",
      "disc/fc-2/add   [None, 512]\n",
      "disc/fc-3/add   [None, 512]\n",
      "disc/fc-4/add   [None, 1]\n",
      "is_reuse: True\n",
      "disc_1/fc-1/add   [256, 512]\n",
      "disc_1/fc-2/add   [256, 512]\n",
      "disc_1/fc-3/add   [256, 512]\n",
      "disc_1/fc-4/add   [256, 1]\n"
     ]
    }
   ],
   "source": [
    "def Generator(n_samples, real_data_, name='gen'):\n",
    "    if FIXED_GENERATOR:\n",
    "        return real_data_ + (1. * tf.random_normal(tf.shape(real_data_)))\n",
    "    else:\n",
    "        with tf.variable_scope(name):\n",
    "            noise = tf.random_normal([n_samples, 2])\n",
    "            output01 = tf_utils.linear(noise, DIM, name='fc-1')\n",
    "            output01 = tf_utils.relu(output01, name='relu-1')\n",
    "            \n",
    "            output02 = tf_utils.linear(output01, DIM, name='fc-2')\n",
    "            output02 = tf_utils.relu(output02, name='relu-2')\n",
    "            \n",
    "            output03 = tf_utils.linear(output02, DIM, name='fc-3')\n",
    "            output03 = tf_utils.relu(output03, name='relu-3')\n",
    "            \n",
    "            output04 = tf_utils.linear(output03, GEN_DIM, name='fc-4')\n",
    "            \n",
    "            return output04\n",
    "        \n",
    "\n",
    "def Discriminator(inputs, is_reuse=True, name='disc'):\n",
    "    with tf.variable_scope(name, reuse=is_reuse):\n",
    "        print('is_reuse: {}'.format(is_reuse))\n",
    "        output01 = tf_utils.linear(inputs, DIM, name='fc-1')\n",
    "        output01 = tf_utils.relu(output01, name='relu-1')\n",
    "\n",
    "        output02 = tf_utils.linear(output01, DIM, name='fc-2')\n",
    "        output02 = tf_utils.relu(output02, name='relu-2')\n",
    "\n",
    "        output03 = tf_utils.linear(output02, DIM, name='fc-3')\n",
    "        output03 = tf_utils.relu(output03, name='relu-3')\n",
    "\n",
    "        output04 = tf_utils.linear(output03, DIS_DIM, name='fc-4')\n",
    "        \n",
    "        return output04\n",
    "    \n",
    "real_data = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "fake_data = Generator(BATCH_SIZE, real_data)\n",
    "\n",
    "disc_real = Discriminator(real_data, is_reuse=False)\n",
    "disc_fake = Discriminator(fake_data)\n",
    "\n",
    "if mode == 'wgan' or mode == 'wgan-gp':\n",
    "    # WGAN loss\n",
    "    disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "    gen_cost = - tf.reduce_mean(disc_fake)\n",
    "\n",
    "    # WGAN gradient penalty\n",
    "    if mode == 'wgan-gp':\n",
    "        alpha = tf.random_uniform(shape=[BATCH_SIZE, 1], minval=0., maxval=1.)\n",
    "        interpolates = alpha*real_data + (1.-alpha) * fake_data\n",
    "        disc_interpolates = Discriminator(interpolates)\n",
    "        gradients = tf.gradients(disc_interpolates, [interpolates][0])\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes - 1)**2)\n",
    "        \n",
    "        disc_cost += LAMBDA * gradient_penalty\n",
    "elif mode == 'gan':\n",
    "    gen_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake, labels=tf.ones_like(disc_fake)))\n",
    "    \n",
    "    disc_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake, labels=tf.zeros_like(disc_fake)))\n",
    "    disc_cost += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real, labels=tf.ones_like(disc_real)))\n",
    "    disc_cost /= 2.\n",
    "    \n",
    "disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='disc')\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='gen')\n",
    "\n",
    "if mode == 'wgan-gp':\n",
    "    disc_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(disc_cost, var_list=disc_vars)\n",
    "    \n",
    "    if len(gen_vars) > 0:\n",
    "        gen_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(gen_cost, var_list=gen_vars)\n",
    "    else:\n",
    "        gen_train_op = tf.no_op()\n",
    "        \n",
    "elif mode == 'wgan':\n",
    "    disc_train_op = tf.train.RMSPropOptimizer(learning_rate=5e-5).minimize(disc_cost, var_list=disc_vars)\n",
    "    \n",
    "    if len(gen_vars) > 0:\n",
    "        gen_train_op = tf.train.RMSPropOptimizer(learning_rate=5e-5).minimize(gen_cost, var_list=gen_vars)\n",
    "    else:\n",
    "        gen_train_op = tf.no_op()\n",
    "        \n",
    "    # build an op to do the weight clipping\n",
    "    clip_bounds = [-0.01, 0.01]\n",
    "    clip_ops = [var.assign(tf.clip_by_value(var, clip_bounds[0], clip_bounds[1])) for var in disc_vars]\n",
    "    clip_disc_weights = tf.group(*clip_ops)\n",
    "    \n",
    "elif mode == 'gan':\n",
    "    disc_train_op = tf.train.AdamOptimizer(learning_rate=2e-4, beta1=0.5).minimize(disc_cost, var_list=disc_vars)\n",
    "    \n",
    "    if len(gen_vars) > 0:                                                        \n",
    "        gen_train_op = tf.train.AdamOptimizer(learning_rate=2e-4, beta1=0.5).minimize(gen_cost, var_list=gen_vars)\n",
    "    else:\n",
    "        gen_train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_image(sess, true_dist, idx):\n",
    "    # generates and saves a plot of the true distribution, the generator, and the critic\n",
    "    N_POINTS = 128\n",
    "    RANGE = 2\n",
    "    \n",
    "    points = np.zeros((N_POINTS, N_POINTS, 2), dtype='float32')\n",
    "    points[:, :, 0] = np.linspace(-RANGE, RANGE, N_POINTS)[:, None]\n",
    "    points[:, :, 1] = np.linspace(-RANGE, RANGE, N_POINTS)[None, :]\n",
    "    points = points.reshape((-1, 2))\n",
    "    \n",
    "    if FIXED_GENERATOR is not True:\n",
    "        samples = sess.run(fake_data, feed_dict={real_data: points})\n",
    "    disc_map = sess.run(disc_real, feed_dict={real_data: points})\n",
    "    \n",
    "    plt.clf()\n",
    "    x = y = np.linspace(-RANGE, RANGE, N_POINTS)\n",
    "    plt.contour(x, y, disc_map.reshape((len(x), len(y))).transpose())\n",
    "    plt.colorbar()  # add color bar\n",
    "    \n",
    "    plt.scatter(true_dist[:, 0], true_dist[:, 1], c='orange', marker='+')\n",
    "    if FIXED_GENERATOR is not True:\n",
    "        plt.scatter(samples[:, 0], samples[:, 1], c='green', marker='*')\n",
    "        \n",
    "    plt.savefig(os.path.join(img_folder, str(idx).zfill(3) + '.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset iterator\n",
    "def inf_train_gen():\n",
    "    if dataset == '8gaussians':\n",
    "        scale = 2.\n",
    "        centers = [(1.,0.), \n",
    "                   (-1.,0.), \n",
    "                   (0., 1.), \n",
    "                   (0.,-1.),\n",
    "                   (1./np.sqrt(2), 1./np.sqrt(2)),\n",
    "                   (1./np.sqrt(2), -1/np.sqrt(2)), \n",
    "                   (-1./np.sqrt(2), 1./np.sqrt(2)), \n",
    "                   (-1./np.sqrt(2), -1./np.sqrt(2))]\n",
    "        \n",
    "        centers = [(scale*x, scale*y) for x, y in centers]\n",
    "        while True:\n",
    "            batch_data = []\n",
    "            for _ in range(BATCH_SIZE):\n",
    "                point = np.random.randn(2) * .02\n",
    "                center = random.choice(centers)\n",
    "                point[0] += center[0]\n",
    "                point[1] += center[1]\n",
    "                batch_data.append(point)\n",
    "                \n",
    "            batch_data = np.array(batch_data, dtype=np.float32)\n",
    "            batch_data /= 1.414  # std\n",
    "            yield batch_data\n",
    "            \n",
    "    elif dataset == '25gaussians':\n",
    "        batch_data = []\n",
    "        for i_ in range(4000):\n",
    "            for x in range(-2, 3):\n",
    "                for y in range(-2, 3):\n",
    "                    point = np.random.randn(2) * 0.05\n",
    "                    point[0] += 2*x\n",
    "                    point[1] += 2*y\n",
    "                    batch_data.append(point)\n",
    "                    \n",
    "        batch_data = np.asarray(batch_data, dtype=np.float32)\n",
    "        np.random.shuffle(batch_data)\n",
    "        batch_data /= 2.828  # std\n",
    "        \n",
    "        while True:\n",
    "            for i_ in range(int(len(batch_data)/BATCH_SIZE)):\n",
    "                yield batch_data[i_*BATCH_SIZE:(i_+1)*BATCH_SIZE]\n",
    "                \n",
    "    elif dataset == 'swissroll':\n",
    "        while True:\n",
    "            batch_data = sklearn.datasets.make_swiss_roll(n_samples=BATCH_SIZE, noise=0.25)[0]\n",
    "            batch_data = batch_data.astype(np.float32)[:, [0, 2]]\n",
    "            batch_data /= 7.5  # stdev plus a little\n",
    "            yield batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\tdisc cost\t0.7139465808868408\n",
      "iter 1000\tdisc cost\t0.6814616771042347\n",
      "iter 2000\tdisc cost\t0.6737550102472305\n",
      "iter 3000\tdisc cost\t0.6920662943720818\n",
      "iter 4000\tdisc cost\t0.6906127047538757\n",
      "iter 5000\tdisc cost\t0.6930880607366562\n",
      "iter 6000\tdisc cost\t0.6931662155985833\n",
      "iter 7000\tdisc cost\t0.6919992100596428\n",
      "iter 8000\tdisc cost\t0.6933745203018189\n",
      "iter 9000\tdisc cost\t0.6926324440836906\n",
      "iter 10000\tdisc cost\t0.6933235999941826\n",
      "iter 11000\tdisc cost\t0.6933176911473274\n",
      "iter 12000\tdisc cost\t0.6923655412197113\n",
      "iter 13000\tdisc cost\t0.6932780802249908\n",
      "iter 14000\tdisc cost\t0.6932716737985611\n",
      "iter 15000\tdisc cost\t0.6932756861448288\n",
      "iter 16000\tdisc cost\t0.6915568546652794\n",
      "iter 17000\tdisc cost\t0.6933772000074386\n",
      "iter 18000\tdisc cost\t0.6933294381499291\n",
      "iter 19000\tdisc cost\t0.6924007230997086\n",
      "iter 20000\tdisc cost\t0.6928844445347786\n",
      "iter 21000\tdisc cost\t0.6933696148395538\n",
      "iter 22000\tdisc cost\t0.6933540980815888\n",
      "iter 23000\tdisc cost\t0.6933285065889359\n",
      "iter 24000\tdisc cost\t0.6918070427775384\n",
      "iter 25000\tdisc cost\t0.6933358488082886\n",
      "iter 26000\tdisc cost\t0.6933039631843567\n",
      "iter 27000\tdisc cost\t0.6932847768068313\n",
      "iter 28000\tdisc cost\t0.6932973915338516\n",
      "iter 29000\tdisc cost\t0.6932247024178505\n",
      "iter 30000\tdisc cost\t0.6932735745310783\n",
      "iter 31000\tdisc cost\t0.6932096065282821\n",
      "iter 32000\tdisc cost\t0.6890769819617272\n",
      "iter 33000\tdisc cost\t0.6923709976673126\n",
      "iter 34000\tdisc cost\t0.6931670038104057\n",
      "iter 35000\tdisc cost\t0.6929483571052552\n",
      "iter 36000\tdisc cost\t0.6932186397910118\n",
      "iter 37000\tdisc cost\t0.6932798113822937\n",
      "iter 38000\tdisc cost\t0.6932742780447007\n",
      "iter 39000\tdisc cost\t0.6931445201039315\n",
      "iter 40000\tdisc cost\t0.6932249497771263\n",
      "iter 41000\tdisc cost\t0.6932100749611855\n",
      "iter 42000\tdisc cost\t0.6929260883331299\n",
      "iter 43000\tdisc cost\t0.6932117487192154\n",
      "iter 44000\tdisc cost\t0.6931842603087425\n",
      "iter 45000\tdisc cost\t0.6931332590579986\n",
      "iter 46000\tdisc cost\t0.6931791034936905\n",
      "iter 47000\tdisc cost\t0.693181702375412\n",
      "iter 48000\tdisc cost\t0.6931661418676376\n",
      "iter 49000\tdisc cost\t0.6932356751561165\n",
      "iter 50000\tdisc cost\t0.6931068043708801\n",
      "iter 51000\tdisc cost\t0.6931518998742103\n",
      "iter 52000\tdisc cost\t0.6929512389302254\n",
      "iter 53000\tdisc cost\t0.69314514118433\n",
      "iter 54000\tdisc cost\t0.6931488544344903\n",
      "iter 55000\tdisc cost\t0.6926609833240509\n",
      "iter 56000\tdisc cost\t0.6931225464940071\n",
      "iter 57000\tdisc cost\t0.693101442694664\n",
      "iter 58000\tdisc cost\t0.6929626023769379\n",
      "iter 59000\tdisc cost\t0.6930847532749176\n",
      "iter 60000\tdisc cost\t0.6928317602872849\n",
      "iter 61000\tdisc cost\t0.6930897635817528\n",
      "iter 62000\tdisc cost\t0.6931442449092865\n",
      "iter 63000\tdisc cost\t0.6931378693580628\n",
      "iter 64000\tdisc cost\t0.6931423845887185\n",
      "iter 65000\tdisc cost\t0.6929416499137878\n",
      "iter 66000\tdisc cost\t0.6930960271954536\n",
      "iter 67000\tdisc cost\t0.6931367915272713\n",
      "iter 68000\tdisc cost\t0.6931659261584282\n",
      "iter 69000\tdisc cost\t0.6931650475263595\n",
      "iter 70000\tdisc cost\t0.6930810611844063\n",
      "iter 71000\tdisc cost\t0.6931314595341682\n",
      "iter 72000\tdisc cost\t0.6931117633581162\n",
      "iter 73000\tdisc cost\t0.6931591765880585\n",
      "iter 74000\tdisc cost\t0.6929754077196121\n",
      "iter 75000\tdisc cost\t0.693103608250618\n",
      "iter 76000\tdisc cost\t0.6931282907724381\n",
      "iter 77000\tdisc cost\t0.6930146722197532\n",
      "iter 78000\tdisc cost\t0.6930771964788437\n",
      "iter 79000\tdisc cost\t0.6931223605871201\n",
      "iter 80000\tdisc cost\t0.693151865184307\n",
      "iter 81000\tdisc cost\t0.693146475315094\n",
      "iter 82000\tdisc cost\t0.6931274979114532\n",
      "iter 83000\tdisc cost\t0.6924246903657914\n",
      "iter 84000\tdisc cost\t0.6930215736031532\n",
      "iter 85000\tdisc cost\t0.6930515897274018\n",
      "iter 86000\tdisc cost\t0.6930611334443092\n",
      "iter 87000\tdisc cost\t0.6926919516324997\n",
      "iter 88000\tdisc cost\t0.6930547666549682\n",
      "iter 89000\tdisc cost\t0.6929932239055634\n",
      "iter 90000\tdisc cost\t0.6930882499814034\n",
      "iter 91000\tdisc cost\t0.6930873312354088\n",
      "iter 92000\tdisc cost\t0.6923705233931542\n",
      "iter 93000\tdisc cost\t0.6930174446702003\n",
      "iter 94000\tdisc cost\t0.6927620051503182\n",
      "iter 95000\tdisc cost\t0.69283284419775\n",
      "iter 96000\tdisc cost\t0.6930583512187004\n",
      "iter 97000\tdisc cost\t0.6930861786007881\n",
      "iter 98000\tdisc cost\t0.6927603552937508\n",
      "iter 99000\tdisc cost\t0.6929180439114571\n",
      "iter 99999\tdisc cost\t0.6930565155662216\n"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    data_gen = inf_train_gen()\n",
    "    \n",
    "    for iter_ in range(ITERS):\n",
    "        batch_data, disc_cost_ = None, None\n",
    "        \n",
    "        # train critic\n",
    "        for i_ in range(CRITIC_ITERS):\n",
    "            batch_data = data_gen.__next__()\n",
    "            disc_cost_, _ = sess.run([disc_cost, disc_train_op], feed_dict={real_data: batch_data})\n",
    "            \n",
    "            if mode == 'wgan':\n",
    "                sess.run(clip_disc_weights)\n",
    "        \n",
    "        # train generator\n",
    "        sess.run(gen_train_op)\n",
    "        \n",
    "        # write logs and svae samples\n",
    "        utils.plot('disc cost', disc_cost_)\n",
    "        \n",
    "        if (np.mod(iter_, FREQ) == 0) or (iter_+1 == ITERS):\n",
    "            utils.flush(img_folder)\n",
    "            generate_image(sess, batch_data, iter_)\n",
    "            \n",
    "        utils.tick()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
